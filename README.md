# ðŸš€ Reinforcement Learning (DA6400)  

This repository contains implementations of various reinforcement learning algorithms as part of the **DA6400** course at **IIT Madras**, instructed by **Prof. Balaraman Ravindran**. The implementations are developed as part of tutorial exercises to understand and analyze reinforcement learning techniques.

---

## ðŸ“Œ Repository Highlights  

ðŸ”¹ **Course:** DA6400 - Reinforcement Learning  
ðŸ”¹ **Instructor:** Prof. Balaraman Ravindran  
ðŸ”¹ **Focus:** Implementation of RL algorithms & experimental analysis  
ðŸ”¹ **Status:** Ongoing updates with new algorithms ðŸš€  

---

## ðŸ“‚ Repository Structure  

###  **Tutorial 1: Multi-Armed Bandit Algorithms**  
The first tutorial explores the **Bandit Problem**, implementing and analyzing different action-selection strategies:  

- âœ… **Random Policy**  
- âœ… **Epsilon-Greedy Algorithm**  
- âœ… **Softmax Exploration**  
- âœ… **Upper Confidence Bound (UCB)**  

ðŸ“Š Comparative analysis of these algorithms is conducted to evaluate their performance in different settings.

### **Tutorial 2: MENACE - A Tic-Tac-Toe Learning System**  
The second tutorial focuses on **MENACE (Matchbox Educable Noughts And Crosses Engine)**, a physical reinforcement learning system designed by **Donald Michie** to play Tic-Tac-Toe.  

- âœ… **Understanding MENACE's Box-based Learning Approach**  
- âœ… **Implementation of MENACE in Python**  
- âœ… **Training MENACE to Improve Gameplay**  
- âœ… **Analysis of MENACE's Learning over Multiple Rounds**  

ðŸ“ˆ This tutorial demonstrates how reinforcement learning principles can be applied even without traditional programming.  

Tutorial 3: Value Iteration & Policy Iteration

The third tutorial implements Value Iteration and Policy Iteration on a standard grid world environment. The grid world consists of 121 cells (11 x 11), where the agent can move deterministically in four directions (up, down, left, right).

Markov Decision Process (MDP) Setup:

States (S): 121 (11 x 11 grid)

Actions (A): 4 (up, down, left, right)

Transition Probability (P): Deterministic

Reward Function (R): -1 at every step

Discount Factor (Î³): 0.9

The objective is to find the optimal policy using:

âœ… Value Iteration

âœ… Policy Iteration

ðŸ“Š The results include visualizing the convergence of values and optimal policies in the grid world.

---

## ðŸ”® Future Updates  

ðŸ”œ **Temporal Difference Learning**  
ðŸ”œ **Monte Carlo Methods**  
ðŸ”œ **Deep Reinforcement Learning**  

Stay tuned for upcoming implementations and insights! ðŸŽ¯

---

## ðŸ“§ Contact

If you have questions, suggestions, or just want to connect, feel free to reach out!

- **Name**: Manoj Kumar.CM  
- **Email**: [manoj.kumar@dsai.iitm.ac.in]  
- **GitHub Profile**: [Manoj Kumar C M](https://github.com/MANOJKUMAR-CM)

Happy coding! ðŸš€


